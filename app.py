# -*- coding: utf-8 -*-
"""first.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWqVIcW2qot8vhI2DJQCWibnU4Y6gulH
"""

import requests

def fetch_robots_txt(url):
    if not url.endswith('/'):
        url += '/'
    robots_url = url + 'robots.txt'
    response = requests.get(robots_url)
    return response.text if response.status_code == 200 else None

def parse_robots(robots_txt):
    rules = {"Allow": [], "Disallow": [], "Crawl-delay": None, "Sitemap": []}
    for line in robots_txt.splitlines():
        line = line.strip()
        if line.startswith('Allow:'):
            rules['Allow'].append(line.split(':', 1)[1].strip())
        elif line.startswith('Disallow:'):
            rules['Disallow'].append(line.split(':', 1)[1].strip())
        elif line.startswith('Crawl-delay:'):
            rules['Crawl-delay'] = line.split(':', 1)[1].strip()
        elif line.lower().startswith('sitemap:'):
            rules['Sitemap'].append(line.split(':', 1)[1].strip())
    return rules

def can_crawl(path, rules):
    for disallowed in rules['Disallow']:
        if path.startswith(disallowed):
            return False
    return True

from bs4 import BeautifulSoup
import requests

def extract_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    titles = [tag.get_text() for tag in soup.find_all('h1')]
    descriptions = [tag.get('content') for tag in soup.find_all('meta', {'name': 'description'})]
    links = [a['href'] for a in soup.find_all('a', href=True)]
    return {'titles': titles, 'descriptions': descriptions, 'links': links}

import time

def fetch_with_retry(url, retries=3):
    for i in range(retries):
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                return response
        except requests.RequestException:
            time.sleep(2)
    return None

from playwright.sync_api import sync_playwright

def extract_with_js(url):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url)
        content = page.content()
        browser.close()
        return BeautifulSoup(content, 'html.parser')

import streamlit as st

def dashboard(summary, extracted_data):
    st.title("Web Crawler Report")
    st.subheader("Crawlability Summary")
    st.json(summary)

    st.subheader("Top Extracted Titles")
    st.write(extracted_data.get('titles', []))

    st.subheader("Recommendations")
    st.markdown("- Use Selenium for JS-heavy websites")
    st.markdown("- Consider obeying robots.txt rules")

import streamlit as st

def dashboard(summary, extracted_data):
    st.title("Web Crawler Report")
    st.subheader("Crawlability Summary")
    st.json(summary)

    st.subheader("Top Extracted Titles")
    for title in extracted_data.get('titles', [])[:10]:
        st.write(f"- {title}")

    st.subheader("Recommendations")
    st.markdown("- Use Selenium or Playwright for JS-heavy websites")
    st.markdown("- Respect robots.txt rules")
    st.markdown("- Consider caching pages to reduce load")